# Artificial Neural Networks

## Motivation
Key idea: We can feed raw features, and the NN can learn good features/representations rather than us having to do do manual feature engineering

Hidden layers correspond to "higher-level" feature extractors and learned representations. They allow for "end-to-end" learning, as they provide the notion of automated feature extraction/representation. Critical for domains like images/audio, but also demonstrated to work well in most areas.

Allows creating complex architectures suitbale for many problems or working with a mix of input types (images, text, etc).

> The price we pay, instead of having to do feature engineering, now have to do NN architecture engineering.
- Look for architectures that work for different tasks
- But we got large performance gains

How are we going to train these models?
[[nn-backpropogation]]

## Related
[[nn-activation-functions]]

[[nn-regularization]]

## References:
1. ML 451 lecture 22

#neural-networks #ml

[//begin]: # "Autogenerated link references for markdown compatibility"
[nn-backpropogation]: nn-backpropogation.md "Backpropagation - Optimizing Neural Networks"
[nn-activation-functions]: nn-activation-functions.md "Activation Functions - ANNs"
[nn-regularization]: nn-regularization.md "Regularization - Neural Netowrks"
[//end]: # "Autogenerated link references"