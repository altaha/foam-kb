# Activation Functions - ANNs

A NN architecture without non-linear activations is equivalent to a single linear function. So adding non-linear activations is a must.

Output of a hidden unit:
$$ h^{(i)} = \phi(W^{(i)} h^{(i-1)} + b^{(i)} )$$

## Sigmoid
$$ \phi(z) = \sigma(z) =  \frac{1}{1 + e^{-z}} $$

The original activation function - this was used in the earliest NN work. A natural function that emerges if we want to map a real valued input into a probabilistic output [0, 1] - works when you want a linear model for binary classification.

Also called logistic function. Arises in the derivation of logistic regression.

> One can think of most basic ANNs as a stack of logistic regression models, where every hidden unit is a probabilistic value depending on the layer prior to it.

Pros
- Easy to differrentiate
- Can be interpreted as a Probability

Cons
- It saturates - for inputs outside the range [-4, 4] it is basically constant and has gradient ~0
    - Problematic: The dynamic range of our hidden unit is limited. e.g: Once it saturates to 1 it doesn't allow you to make the output biggger if you want it to be more positive. One of the main reasons people started moving away.

### Properties
It is easy to differrentiate:
$$ \sigma'(z) =  \sigma(z)(1 - \sigma(z))$$

Symmetry about (0, 0.5)
$$ \sigma(z) =  (1 - \sigma(-z)) $$

## Tanh - Huperbolic tangent
$$ \phi(z) = \text{tanh} \ z =  \frac{e^z - e^{-z}}{e^z + e^{-z}} $$

Pros
- Easy to differrentiate
$$ \text{tanh}' z = 1 - \text{tanh}^2 z  $$
- Can also be interpreted as a Probability - normalize and interpret as a probability.

Cons
- Slighlty less prone to "saturation" than sigmoid but still has fixed range. Possibly because it has a steaper curve.

## ReLU - Rectified Linear Unit
$$ \phi(z) = \text{ReLU} (z) =  max(z, 0) $$
Is now the *de facto* standard in Deep Learning.

Pros
- Unbounded range - so it never saturates.
  - > Though that's only on one side, it's ok because you cal multiply the output by -1 to get a "negative ReLU".
- Demosntrated very strong empirical results.
- Very simple  derivative (1 or 0)
  - Technically, it is a sub-gradient

Cons
- [[todo]]: I need to understand the idea of dead-neurons

## Softplus - one of many variants of ReLUs
$$ \phi(z) = \text{softplus} (z) = ln(1 + e^z) $$
Pros
- A smooth approximation to Relu - a theoretical advantage
- Avoids ReLU going completely off

Cons
- Expnesive derivative compared to ReLU. Perhaps it's not worth the extra flops required to compute its derivative.

Other variants: leaky Relu, etc

## Output activation functions:
The above discussion applies to hidden units. But for the output, the choice of activation is typically determined by the task. So usually a linear function for regression, sigmoid for binary classification, softmax for multiclass, etc.

[//begin]: # "Autogenerated link references for markdown compatibility"
[todo]: ../../todo.md "TODOs"
[//end]: # "Autogenerated link references"