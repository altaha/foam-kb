# Recurrent Neural Networks

## Concepts
TBD

## References:
### Unreasonable effectiveness of RNNs [url](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - [[andrej-karpathy]]
- Provides a basic and intuitive explanation of RNN models
- Shows some very neat demos of character level RNN language models (learns linux source code, etc)
- Provides some good links to research directions from that time (2015), includes references to [[dl-attention]]
> The first convincing example of moving towards these directions was developed in DeepMind’s [Neural Turing Machines](http://arxiv.org/abs/1410.5401) paper. This paper sketched a path towards models that can perform read/write operations between large, external memory arrays and a smaller set of memory registers (think of these as our working memory) where the computation happens. Crucially, the NTM paper also featured very interesting memory addressing mechanisms that were implemented with a (soft, and fully-differentiable) attention model. The concept of soft attention has turned out to be a powerful modeling feature and was also featured in [Neural Machine Translation by Jointly Learning to Align and Translate](http://arxiv.org/abs/1409.0473) for Machine Translation and [Memory Networks](http://arxiv.org/abs/1503.08895) for (toy) Question Answering. In fact, I’d go as far as to say that: "**The concept of attention is the most interesting recent architectural innovation in neural networks.**"

- Also references some reinforcement learning based techniques to handle non-differentiable methods. Something called REINFORCE. I need to further look into this idea of using RL to handle non-differentiable stuffs [[todo]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[andrej-karpathy]: andrej-karpathy.md "Andrej Karpathy"
[dl-attention]: dl-attention.md "Dl Attention"
[todo]: ../../todo.md "Todo"
[//end]: # "Autogenerated link references"