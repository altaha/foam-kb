# BigBird: Tranformers for longer Sequences

BigBird is another extension to Tranformers [[dl-transformers]], that aims to eliminate the quadratic cost of a full attention model.

Vanilla [[dl-attention]] in transformers and in [[dl-bert]] is is fully connected between input and output tokens. Given n tokens, it requires O(n^2) connections. This quadratic cost limits the size of sequences that can be processed (Bert is limited to 512 tokens).

BigBird name: another character from Sesame street (like Elmo and Bert).

Achieving linear attention is an active area of research. Similar methods in this direction are Reformer [[dl-transformers-reformer]], Performer [[dl-transformers-performer]], Longformer [[dl-transformers-longformer]].

BigBird relies on the following 3 techniques to achieve linear O(n) attention:
1. **Random Attention**: Every node connects to r nodes selected at random. They argue that a random walk through a graph can connect all nodes in logarithmic order (todo: understand this more)
2. **Window Attention**: every node connected to surrounding nodes, within a w window size. Intuitivly, we expect nearby nodes to be important. This also matches empirical evidnce.
3. **Global Attention**: a few of the nodes g are connected to all the nodes. This includes the special [CLS] token. Having global attention nodes means any two nodes are at most 1 hop away.

These three techniques provide O(n) attention, but with a constant multiple of O(n) based on r, w, and g. This multiple is not small, but should provide good savings relative to quadratic, for large n.

In fact, **BigBird uses the same techniques as Longformer (2 and 3), and just adds to it Random Attention (1)**.

The paper provides proofs that transformers using these techniques are Universal Approximators [[ml-universal-approximator]] and are Turing complete [[cs-turing-complete]].

The paper also discusses engineering tweaks to make it work with good performance. To benefit from vectorization and memory access patterns, random attention operates on blocks instead of single nodes. They do the same with the stride of window attention.

## Results
With this technique, they trained a transformer model that can operate over sequences of length 4096 (8 times the 512 in Bert). They also showed strong performance in NLU tasks (sota for some tasks).

They also did some experiments on genomics tasks, which I guess typically have longer sequences (todo: review what they did).

This shows that:
1) These O(n) attention are able to provide good performance, so theey are valid.
2) 4096 length is good, but is only 8x increase. So still not there to be able to work on much longer sequences.

## References
Papers explained [youtube video](https://www.youtube.com/watch?v=WVPE62Gk3EM)

[//begin]: # "Autogenerated link references for markdown compatibility"
[dl-transformers]: dl-transformers.md "Dl Transformers"
[dl-attention]: dl-attention.md "Dl Attention"
[dl-transformers-reformer]: dl-transformers-reformer.md "Dl Transformers Reformer"
[//end]: # "Autogenerated link references"