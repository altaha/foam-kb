# Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach

[Paper on Arxiv](https://arxiv.org/pdf/2010.07835.pdf)

## Abstract
- Pretrained Langague Models (LMs) achieved enormous success in NLP. But they still require excessive labeled data for Fine-Tuning. The paper studies finetuning LMs using only weak supervision, without any labeled data.
- Since LMs have a high capacity to overfit, finetuning on weak labels makes them prone to overfitting the noise in the weak labels.
- Paper develops COSINE: "a contrastive self-training framework", to enable fine-tuning LMs with weak supervision.
- COSINE primarily relies on Contrastive Regularization, and Confidence-based Reweighting. It improves model fitting while suppressing error-propagation.
- Experiments on NLP tasks demonstrate large improvement over weak-supervision baselines. And competitive with fully-supervised fine-tuning.
- [Implementation](https://github.com/yueyu1030/COSINE)

## Introduction
Finetuning pre-trained LMs like BERT is sota on many NLP tasks. These approaches stack task-specific layers on top of pre-trained models, then fine-tune using supervised task-specific data.  Fine-Tuning adapts the semantic and syntactic knowledge in the LM to the target task.

A bottleneck is that fine-tuning requires labeled data. Fine-tuning performance often degrades when labeled data is scarce, and results in severe overfitting ([Xie et al, 2019](http://arxiv.org/abs/1904.12848)). To relieve the bottlence, we can try using weak supervision. Weakly labeled data is typically cheaper to collect, for example using simple rules (e.g semantic rules like: 'terrible' -> Negative sentiment).

Fine-tuing LMs with weak-supervision is nontrivial. Excessive label noise (wrong labels, and limited coverage) are common. Common fine-tuning appraoches are not designed for noisy data, and easy to overfit on the noise.

Existing works on tackling label noise are flawed. Lists many references: Snorkel (Ratner et al. (2020)); Varma and RÃ© (2018); (Aina et al., 2019); (Luo et al., 2017; Wang et al., 2019b); (Awasthi et al., 2020; Ren et al., 2020).. Self-training (Rosenberg et al., 2005; Lee, 2013) is a good tool for fine-tuning LMs, as it augments the trainng set with unlabled data by generating pseudo-labels. But they suffer from error-propagation from wrong pseudo labels.

Propose new algorithm, COSINE, which fine tunes pre-trained LMs using only weak supervision.
- COSINE leverages both weakly labeled and unlabled data, and suppresses label noise via Contrastive Self-training. Contrastive self-training regularizes the feature-space by pushing samples with the same pseudo-labels close, while pulling samples with different pseudo-labels apart. Such regularization enforces sample representations of samples from different classes to be more distinguishable, so the classifier can make better decisions.
- To suppress label noise propagation during contrastive self-training, they propose confidence-based sample reweighting and regularization methods. The reweighting strategy reduces the effect of wrong predictions, by emphasizing samples with high prediction confidence which are more likely to be correct. Confidence regularization encourages smoothness over model predictions, such that no prediction can be over-confident, and therefore reduces the influence of wrong pseudo-labels

The model is flexible and can be naturally extended to semi-supervised learning, where a small set of clean labels is available. Also, no assumptions are made about the nature of the weak labels, so COSINE can handle various types of label noise, including biased labels and randomly corrupted labels. Biased labels are usually generated by semantic rules, whereas corrupted labels are often produced by crowd-sourcing.

## Background (Problem Formulation)

**Weak Supervision:**

From weak supervision sources, each of the input samples $x \in \mathcal{X}$ is given a label $y \in \mathcal{Y} \cup \ \{\empty\}$
- Where $\mathcal{Y}$ is the set of labels, and $\empty$ denotes the sampel is not matched by any rules.
- For samples given multiple labels, a label is determined via majority vote.

**Weakly-supervised classification Problem**

Given weakly labeled samples: $\mathcal{X}_l = \{(x_i, y_i)\}_{i=1}^{L}$, and unalbeled samples: $\mathcal{X}_u = \{x_j\}_{j=1}^{U}$

We seek to learn a classifier
$$
f(x; \theta): \mathcal{X} \rightarrow \mathcal{Y} \\
\text{Where} \ \mathcal{X} = \mathcal{X_l} \cup \mathcal{X_u} \ \text{, denotes all samples.} \\
\text{and} \ \mathcal{Y} = \{1, 2, ..., C\} \ \text{is the label set, and C is the number of classes.} \\
$$

## Method

The classifier consists of two parts: $f = g \circ \text{BERT}$
- BERT is a pre-trained LM (but could be a palceholder for other models) that outputs hidden representations. The paper uses RoBERTa.
- g is a task specific classification head that outputs a C-dimensional vector of class probabililties.

COSINE Framework: shown in figure 1.
1. **Initiatlization with Weakly Labeled Data**: Fine-tune $f(. ; x)$ with weakly labeled data by optimizing Cross-Entropy loss.
$$\min_\theta \frac{1}{\mathcal{X_l}} \sum_{(x_i, y_i) \in \mathcal{X_l}} CE(f(x_i; \theta), y_i)$$
Use early-stopping to prevent overfitting to weak labels noise. However, early stopping causes underfitting, so contrastive self-learning is used to resolve the issue.

2. **Contrastive Self-learning with All Data**:
The goal of contrastive self-training is to leverage all data (both labeled and unlabeled) for fine-tuning, as well as to reduce the error propagation of wrongly labelled data. We generate pseudo-labels for the unlabeled data and incorporate them into the training set. To reduce error propagation, we introduce contrastive representation learning (Sec. 3.2) and confidence-based sample reweighting and regularization (Sec. 3.3). We update the pseudo-labels (denoted by  $\tilde{y}$) and the model iteratively. The procedures are summarized in Algorithm 1.

2.1 **Update $\tilde{y}$ with the current $\theta$**:
To generate the pseudo-labels for each sample $x \in \mathcal{X}$, a straightforward way is to use hard-labels (hard assign to highest entry in class probabilities vector):

$$ \tilde{y}_{hard} = \argmax_{j \in \mathcal{Y}} [f(x; \theta)]_j $$
However, hard labels will propagate labeling mistakes and make updating difficult.

Instead, for each sample $x \in \text{batch} \ \mathcal{B}$, generate soft labels $\tilde{y} \in \mathbb{R}^C$ based on current model:
$$ \tilde{y}_{j} = \frac{[f(x; \theta)]_j^2 / f_j}{\sum_{j'\in \mathcal{Y}} [f(x; \theta)]_{j'}^2 / f_{j'} } $$
Where $f_{j} = \sum_{x'\in \mathcal{B}} [f(x'; \theta)]_{j}^2$ is the sum over soft frequencies of class j. The soft-labels guarantee that the error propogated to the model update step will be smaller than using hard-labels.

2.2 **Update $\theta$ with the current $\tilde{y}$**:
We update the model parameters $\theta$ by minimizing
$$ \mathcal{L}(\theta; \tilde{y}) = \mathcal{L}_c(\theta; \tilde{y}) + \mathcal{R}_1(\theta; \tilde{y}) + \lambda \mathcal{R}_2(\theta) $$
where $\mathcal{L}_c$ is the classification loss (Sec. 3.3), $\mathcal{R}_1(\theta; \tilde{y})$ is the contrastive regularizer (Sec. 3.2), $\mathcal{R}_2(\theta)$ is the confidence regularizer (Sec. 3.3), and $\lambda$ is the hyper-parameter for the regularization.

## Contrastive Learning on Samples Pairs

The key ingredient of contrastive self-training is to learn representations that encourage data within the same class to have similar representations and keep data in different classes separated. Specifically, first select high-confidence samples (Sec. 3.3) $\mathcal{C}$ from $\mathcal{X}$, then define similarity for each pair $x_i, x_j \in \mathcal{C}$ as:
$$
W_{ij} = \Big\{ 1, \text{if} \argmax_{k \in \mathcal{Y}} [\tilde{y}_i]_k = \argmax_{k \in \mathcal{Y}} [\tilde{y}_j]_k \\
W_{ij} = \Big\{ 0, \text{otherwise}
$$
where $\tilde{y}_i, \tilde{y}_j$ are the soft pseudo-labels (Eq. 3) for $x_i, x_j$, respectively. For each $x \in \mathcal{C}$, we calculate its representation $v = \text{BERT}(x) \in \mathbb{R}^d$. Then we define the contrastive regularizer as:
$$
\mathcal{R}_1(\theta; \tilde{y}) = \sum_{(x_i, x_j) \in \mathcal{C}\times \mathcal{C}}{l(v_i, v_j, W_{ij})} \\
\text{where}: l = W_{ij}d_{ij}^2 + (1 - W_{ij})[\max(0, \gamma - d_{ij})]^2
$$
Here, $l$ is the contrastive loss (Chopra et al.), $d_{ij}$ is the distance between $v_i$ and $v_j$, and $\gamma$ is a pre-defined margin hyperparameter.

For samples from the same class, i.e. $W_{ij} = 1$, Eq. 6 penalizes the distance between them, and for samples from different classes, the contrastive loss is large if their distance is small. In this way, the regularizer enforces similar samples to be close while keeping dissimilar samples apart by at least $\gamma$. Figure 2 illustrates the contrastive representations.  We can see that our method produces clear inter-class boundaries and small intra-class distances, which eases the classification tasks.

## Confidence Based Sample Reweighting and Regularization.
Contrastive representations yield better decision boundaries, but they require samples with high-quality pseudo-labels.

In this section, we introduce reweighting and regularization methods to suppress error propagation and refine pseudo-label qualities.

**Sample Reweighting**: In the classification task, samples with high prediction confidence are more likely to be classified correctly than those with low confidence. Therefore, we further reduce label noise propagation by a confidence-based sample reweighting scheme. For each sample $x$ with the soft pseudo-label $\tilde{y}$, we assign $x$ with a weight $\omega(x)$ defined by:
$$
\omega = 1 - \frac{H(\tilde{y})}{log(C)}, H(\tilde{y}) = - \sum_{i=1}^C{\tilde{y}_i \log \tilde{y}_i}
$$
where $0 \le H(\tilde{y}) \le \log(C)$ is the entropy of $\tilde{y}$. Notice that if the prediction confidence is low then entropy $H(\tilde{y})$ will be high and the sample weight $\omega(x)$ will be small, and vice versa. The hyperparameter threshold $\xi$ is used to select high confidence samples $\mathcal{C}$ from each batch $\mathcal{B}$ as
$$ C = \{ x \in \mathcal{B} | \omega(x) \ge \xi \} $$

Then we define the loss function as
$$ \mathcal{L}_c(\theta, \tilde{y}) = \frac{1}{|C|} \sum_{x \in \mathcal{C}}{\omega(x) \mathcal{D}_{KL}(\tilde{y} || f(x; \theta))} $$
Where $\mathcal{D}_{KL}$ is the Kullback-Leibler Divergence.

**Confidence Regularization**

The sample reweighting approach promotes high confidence samples during contrastive self-training. However, this strategy relies on wrongly-labeled samples to have low confidence, which may not be true unless we prevent over-confident predictions. To this end, we propose a confidence-based regularizer that encourages smoothness over predictions, defined as
$$ \mathcal{R}_2(\theta) = \frac{1}{|C|} \sum_{x \in \mathcal{C}}{\mathcal{D}_{KL}(\mathbf{u} || f(x; \theta))} $$
Where $\mathbf{u}_i = 1/C, \text{for} \ i = 1, 2, ..., C$. This basically regularizes divergence from a uniform classification distribution, which constitutes a regularization to prevent over-confident predictions and leads to better generalization.




## References:
1. ([Xie et al, 2019](http://arxiv.org/abs/1904.12848)). Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2019. Unsupervised data augmentation for consistency training. CoRR, abs/1904.12848
2.